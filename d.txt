""" Automation Module """
import os
import sys
import json
import shutil
import logging
import yaml
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SPEC_DIR = os.getenv("SPEC_DIR", "../AKS-Cluster-Specs")
IAC_DIR = os.getenv("IAC_DIR", "../AKS-Cluster-IaC")

class Automation:
    def __init__(self, spec_dir=SPEC_DIR, iac_dir=IAC_DIR):
        self.spec_dir = spec_dir
        self.iac_dir = iac_dir

    def load_template(self, spec_name, env):
        """Load YAML template based on the spec name and env"""
        try:
            template_file = f"{env}.yaml"
            template_path = os.path.join(
                self.spec_dir, "specifications", spec_name, template_file
            )
            if not os.path.exists(template_path):
                raise FileNotFoundError(f"Template not found: {template_path}")
            with open(template_path, "r", encoding='utf-8') as file:
                template = yaml.safe_load(file)
            return template
        except FileNotFoundError as e:
            logger.error("%s", e)
            raise
        except yaml.YAMLError as e:
            logger.error("Error loading YAML template: %s", template_path)
            raise ValueError(
                f"Error loading YAML template: {template_path}") from e

    def merge_spec_with_template(self, spec, template):
        """Merge JSON spec with YAML template."""
        try:
            template.update(spec)
            return template
        except KeyError as e:
            logger.error("Missing required spec key: %s", e)
            raise

    def save_to_iac_repo(self, merged_spec, output_filename, cluster_name):
        """Save merged spec to the IaC repository"""
        output_path = os.path.join(
            self.iac_dir, "clusters", cluster_name, "definitions", output_filename
        )
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding='utf-8') as file:
            yaml.dump(merged_spec, file)
        logger.info("Cluster definition saved to: %s", output_path)

    def copy_configuration_files(self, spec_name, cluster_name):
        """Copy config files to IaC repository."""
        config_source_dir = os.path.join(self.spec_dir, "configurations", spec_name)
        config_target_dir = os.path.join(
            self.iac_dir, "clusters", cluster_name, "configurations"
        )

        if not os.path.exists(config_source_dir):
            logger.warning("No configuration files found for spec: %s", spec_name)
            return

        os.makedirs(config_target_dir, exist_ok=True)

        for filename in os.listdir(config_source_dir):
            full_file_name = os.path.join(config_source_dir, filename)
            if os.path.isfile(full_file_name):
                shutil.copy(full_file_name, config_target_dir)
                logger.info("Copied %s to %s", filename, config_target_dir)

    def validate_spec(self, spec):
        """Validate required keys are present in the spec"""
        required_keys = [
            "clusterName",
            "env",
            "workerNodeCount",
            "controlPlaneCount",
            "region",
            "spec",
            "kubernetesVersion",
            "networkPolicy",
            "storageClass",
        ]
        for key in required_keys:
            if key not in spec:
                raise KeyError(f"Missing required key in spec: {key}")

    def run(self, json_spec_path, spec_name):
        """Main function to run the automation script"""
        try:
            with open(json_spec_path, "r", encoding='utf-8') as file:
                json_spec = json.load(file)

            self.validate_spec(json_spec)

            env = json_spec.get("env", "dev")
            cluster_name = json_spec.get("clusterName")
            template = self.load_template(spec_name, env)
            merged_spec = self.merge_spec_with_template(json_spec, template)

            output_filename = f"{cluster_name}-cluster-definition.yaml"
            self.save_to_iac_repo(merged_spec, output_filename, cluster_name)
            self.copy_configuration_files(spec_name, cluster_name)
        except FileNotFoundError as e:
            logger.error(e)
        except json.JSONDecodeError:
            logger.error("Error decoding JSON file: %s", json_spec_path)
        except KeyError as e:
            logger.error(e)
        except ValueError as e:
            logger.error(e)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        logger.error("Usage: python automation.py <json_spec_path> <spec_name>")
        sys.exit(1)

    json_spec_path = sys.argv[1]
    spec_name = sys.argv[2]

    automation = Automation()
    automation.run(json_spec_path, spec_name)


""" Main Module """
import os
import sys
import json
import subprocess
import logging
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SPECS_DIR = os.getenv("SPECS_DIR", "ClusterInputFolder")
AUTOMATION_SCRIPT = os.getenv("AUTOMATION_SCRIPT", "automation.py")

class SpecProcessor:
    def __init__(self, specs_dir=SPECS_DIR, automation_script=AUTOMATION_SCRIPT):
        self.specs_dir = specs_dir
        self.automation_script = automation_script

    def process_json_specs(self, json_file, spec_name):
        """Process JSON specs and call automation script"""
        json_spec_path = os.path.join(self.specs_dir, json_file)
        self.process_single_spec(json_spec_path, spec_name)

    def process_single_spec(self, json_spec_path, spec_name):
        """Process single JSON spec file."""
        try:
            with open(json_spec_path, "r", encoding='utf-8') as file:
                json_spec = json.load(file)

            process = subprocess.run(
                ["python", self.automation_script, json_spec_path, spec_name],
                check=True
            )
            if process.returncode != 0:
                logger.error("Error running automation script: %s", process.returncode)
        except FileNotFoundError:
            logger.error("File not found: %s", json_spec_path)
        except json.JSONDecodeError:
            logger.error("Error decoding JSON file: %s", json_spec_path)
        except subprocess.CalledProcessError as e:
            logger.error("Error running automation script: %s", e)

def main():
    """Main function to run the main script"""
    if len(sys.argv) != 3:
        logger.error("Usage: python main.py <spec_name> <json_input_file>")
        sys.exit(1)

    spec_name = sys.argv[1]
    json_file = sys.argv[2]

    if not os.path.exists(SPECS_DIR):
        logger.error("Specs directory does not exist: %s", SPECS_DIR)
        return

    processor = SpecProcessor()
    with ThreadPoolExecutor() as executor:
        executor.submit(processor.process_json_specs, json_file, spec_name)

if __name__ == "__main__":
    main()




import pytest
import os
import json
import yaml
from automation import Automation

@pytest.fixture
def valid_spec():
    return {
        "clusterName": "test-cluster",
        "env": "dev",
        "workerNodeCount": 3,
        "controlPlaneCount": 1,
        "region": "us-west",
        "spec": {},
        "kubernetesVersion": "1.18.14",
        "networkPolicy": "calico",
        "storageClass": "standard"
    }

def test_load_template(valid_spec):
    automation = Automation()
    template = automation.load_template("test-spec", valid_spec["env"])
    assert isinstance(template, dict)

def test_merge_spec_with_template(valid_spec):
    automation = Automation()
    template = {"dummy_key": "dummy_value"}
    merged_spec = automation.merge_spec_with_template(valid_spec, template)
    assert "dummy_key" in merged_spec
    assert merged_spec["dummy_key"] == "dummy_value"

def test_save_to_iac_repo(valid_spec):
    automation = Automation()
    merged_spec = valid_spec
    automation.save_to_iac_repo(merged_spec, "test-output.yaml", "test-cluster")
    output_path = os.path.join(automation.iac_dir, "clusters", "test-cluster", "definitions", "test-output.yaml")
    assert os.path.exists(output_path)

def test_copy_configuration_files():
    automation = Automation()
    automation.copy_configuration_files("test-spec", "test-cluster")
    config_target_dir = os.path.join(automation.iac_dir, "clusters", "test-cluster", "configurations")
    assert os.path.exists(config_target_dir)



# Environment Variables
SPEC_DIR=/path/to/your/AKS-Cluster-Specs
IAC_DIR=/path/to/your/AKS-Cluster-IaC
SPECS_DIR=/path/to/your/ClusterInputFolder
AUTOMATION_SCRIPT=/path/to/your/src/automation.py




# Dependencies
pyyaml==6.0
python-dotenv==1.0.0
pytest==7.4.0









# AKS Cluster Automation

This repository contains scripts and automation for deploying AKS clusters using a specified configuration and template.

## Folder Structure

- `ClusterInputFolder/`: Contains JSON input files for cluster specifications.
- `AKS-Cluster-Specs/`: Contains specifications, definitions, and configurations for clusters.
- `src/`: Contains the main Python scripts for automation.

## Scripts

- `main.py`: Main entry point for processing JSON input files.
- `automation.py`: Handles the creation and saving of cluster definitions and configuration files.

## Running Locally

To run the automation scripts locally, use the following command:

```sh
python src/main.py <spec_name> <json_input_file>








name: Automation Workflow

on:
  push:
    branches:
      - main
    paths:
      - 'AKS-Cluster-Provisioner/ClusterInputFolder/*.json'

jobs:
  trigger-automation:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r AKS-Cluster-Provisioner/requirements.txt

      - name: Run automation script
        run: |
          python AKS-Cluster-Provisioner/main.py shared AKS-Cluster-Provisioner/ClusterInputFolder/input1.json
          python AKS-Cluster-Provisioner/main.py specific AKS-Cluster-Provisioner/ClusterInputFolder/input2.json
          python AKS-Cluster-Provisioner/main.py special AKS-Cluster-Provisioner/ClusterInputFolder/input3.json





import os
import sys
import json
import yaml
import shutil
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

SPEC_DIR = '../AKS-Cluster-Specs'
IAC_DIR = '../AKS-Cluster-IaC'

def load_template(spec_name, env):
    """Load the YAML template based on the spec name and environment."""
    try:
        template_file = f"{env}.yaml"
        template_path = os.path.join(SPEC_DIR, 'specifications', spec_name, template_file)
        if not os.path.exists(template_path):
            raise FileNotFoundError(f"Template not found: {template_path}")
        with open(template_path, 'r') as file:
            template = yaml.safe_load(file)
        return template
    except FileNotFoundError as e:
        logger.error(e)
        raise
    except yaml.YAMLError as e:
        logger.error(f"Error loading YAML template: {template_path}")
        raise ValueError(f"Error loading YAML template: {template_path}") from e

def merge_spec_with_template(spec, template):
    """Merge the JSON spec with the YAML template."""
    try:
        template.update(spec)
        return template
    except KeyError as e:
        logger.error(f"Missing required spec key: {e}")
        raise

def save_to_iac_repo(merged_spec, output_filename, cluster_name):
    """Save the merged spec to the IaC repository."""
    output_path = os.path.join(IAC_DIR, 'clusters', cluster_name, 'definitions', output_filename)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as file:
        yaml.dump(merged_spec, file)
    logger.info(f"Cluster definition saved to: {output_path}")

def copy_configuration_files(spec_name, cluster_name):
    """Copy configuration files to the appropriate directory in the IaC repository."""
    config_source_dir = os.path.join(SPEC_DIR, 'configurations', spec_name)
    config_target_dir = os.path.join(IAC_DIR, 'clusters', cluster_name, 'configurations')

    if not os.path.exists(config_source_dir):
        logger.warning(f"No configuration files found for spec: {spec_name}")
        return

    os.makedirs(config_target_dir, exist_ok=True)

    for filename in os.listdir(config_source_dir):
        full_file_name = os.path.join(config_source_dir, filename)
        if os.path.isfile(full_file_name):
            shutil.copy(full_file_name, config_target_dir)
            logger.info(f"Copied {filename} to {config_target_dir}")

def validate_spec(spec):
    """Validate that the required keys are present in the spec."""
    required_keys = [
        'clusterName', 'env', 'workerNodeCount', 'controlPlaneCount', 'region',
        'spec', 'kubernetesVersion', 'networkPolicy', 'storageClass'
    ]
    for key in required_keys:
        if key not in spec:
            raise KeyError(f"Missing required key in spec: {key}")

def main():
    """Main function to run the automation script."""
    if len(sys.argv) != 3:
        logger.error("Usage: python automation.py <json_spec_path> <spec_name>")
        sys.exit(1)

    json_spec_path = sys.argv[1]
    spec_name = sys.argv[2]

    try:
        with open(json_spec_path, 'r') as file:
            json_spec = json.load(file)

        validate_spec(json_spec)

        env = json_spec.get('env', 'dev')
        cluster_name = json_spec.get('clusterName')
        template = load_template(spec_name, env)
        merged_spec = merge_spec_with_template(json_spec, template)

        output_filename = f"{cluster_name}-cluster-definition.yaml"
        save_to_iac_repo(merged_spec, output_filename, cluster_name)
        copy_configuration_files(spec_name, cluster_name)
    except FileNotFoundError as e:
        logger.error(e)
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON file: {json_spec_path}")
    except KeyError as e:
        logger.error(e)
    except ValueError as e:
        logger.error(e)

if __name__ == "__main__":
    main()












import os
import sys
import json
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_json_specs(specs_dir, json_file, spec_name):
    """Process JSON specs and call the automation script."""
    json_spec_path = os.path.join(specs_dir, json_file)
    process_single_spec(json_spec_path, spec_name)

def process_single_spec(json_spec_path, spec_name):
    """Process a single JSON spec file."""
    try:
        with open(json_spec_path, 'r') as file:
            json_spec = json.load(file)

        temp_json_path = 'temp_spec.json'
        with open(temp_json_path, 'w') as temp_file:
            json.dump(json_spec, temp_file)

        subprocess.run(['python', 'src/automation.py', temp_json_path, spec_name], check=True)
    except FileNotFoundError:
        logger.error(f"File not found: {json_spec_path}")
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON file: {json_spec_path}")
    except subprocess.CalledProcessError as e:
        logger.error(f"Error running automation script: {e}")

def main():
    """Main function to run the main script."""
    if len(sys.argv) != 3:
        logger.error("Usage: python main.py <spec_name> <json_input_file>")
        sys.exit(1)
    
    spec_name = sys.argv[1]
    json_file = sys.argv[2]

    specs_dir = 'ClusterInputFolder'
    if not os.path.exists(specs_dir):
        logger.error(f"Specs directory does not exist: {specs_dir}")
        return

    process_json_specs(specs_dir, json_file, spec_name)

if __name__ == "__main__":
    main()
























import os
import sys
import json
import yaml
import shutil
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_template(spec_name, env):
    try:
        template_file = f"{env}.yaml"
        template_path = os.path.join(f'../AKS-Cluster-Specs/specifications/{spec_name}', template_file)
        if not os.path.exists(template_path):
            raise FileNotFoundError(f"Template not found: {template_path}")
        with open(template_path, 'r') as file:
            template = yaml.safe_load(file)
        return template
    except FileNotFoundError as e:
        raise e
    except yaml.YAMLError:
        raise ValueError(f"Error loading YAML template: {template_path}")

def merge_spec_with_template(spec, template):
    try:
        for key, value in spec.items():
            if key in template:
                template[key] = value
            else:
                template[key] = value
        return template
    except KeyError as e:
        raise KeyError(f"Missing required spec key: {e}")

def save_to_iac_repo(merged_spec, output_filename, cluster_name):
    output_path = os.path.join(f'../AKS-Cluster-IaC/clusters/{cluster_name}/definitions', output_filename)
    if os.path.exists(output_path):
        logger.info(f"Cluster definition already exists: {output_path}")
        return

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as file:
        yaml.dump(merged_spec, file)
    logger.info(f"Cluster definition saved to: {output_path}")

def copy_configuration_files(spec_name, cluster_name):
    config_source_dir = os.path.join(f'../AKS-Cluster-Specs/configurations/{spec_name}')
    config_target_dir = os.path.join(f'../AKS-Cluster-IaC/clusters/{cluster_name}/configurations')

    if not os.path.exists(config_source_dir):
        logger.warning(f"No configuration files found for spec: {spec_name}")
        return

    os.makedirs(config_target_dir, exist_ok=True)

    for filename in os.listdir(config_source_dir):
        full_file_name = os.path.join(config_source_dir, filename)
        if os.path.isfile(full_file_name):
            shutil.copy(full_file_name, config_target_dir)
            logger.info(f"Copied {filename} to {config_target_dir}")

def main():
    if len(sys.argv) != 3:
        logger.error("Usage: python automation.py <json_spec_path> <spec_name>")
        sys.exit(1)

    json_spec_path = sys.argv[1]
    spec_name = sys.argv[2]

    try:
        with open(json_spec_path, 'r') as file:
            json_spec = json.load(file)

        validate_spec(json_spec)

        env = json_spec.get('env', 'dev')
        cluster_name = json_spec.get('clusterName')
        template = load_template(spec_name, env)
        merged_spec = merge_spec_with_template(json_spec, template)

        output_filename = f"{cluster_name}-cluster-definition.yaml"
        save_to_iac_repo(merged_spec, output_filename, cluster_name)
        copy_configuration_files(spec_name, cluster_name)
    except FileNotFoundError as e:
        logger.error(e)
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON file: {json_spec_path}")
    except KeyError as e:
        logger.error(e)
    except ValueError as e:
        logger.error(e)

def validate_spec(spec):
    required_keys = ['clusterName', 'env', 'workerNodeCount', 'controlPlaneCount', 'region', 'spec', 'kubernetesVersion', 'networkPolicy', 'storageClass']
    for key in required_keys:
        if key not in spec:
            raise KeyError(f"Missing required key in spec: {key}")

if __name__ == "__main__":
    main()




apiVersion: v1
kind: ConfigMap
metadata:
  name: example-config
  namespace: default
data:
  example.key: "example value"




import os
import sys
import json
import yaml
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_template(spec_name, env):
    try:
        template_file = f"{env}.yaml"
        template_path = os.path.join(f'../AKS-Cluster-Specs/specifications/{spec_name}', template_file)
        if not os.path.exists(template_path):
            raise FileNotFoundError(f"Template not found: {template_path}")
        with open(template_path, 'r') as file:
            template = yaml.safe_load(file)
        return template
    except FileNotFoundError as e:
        raise e
    except yaml.YAMLError:
        raise ValueError(f"Error loading YAML template: {template_path}")

def merge_spec_with_template(spec, template):
    try:
        for key, value in spec.items():
            if key in template:
                template[key] = value
            else:
                template[key] = value
        return template
    except KeyError as e:
        raise KeyError(f"Missing required spec key: {e}")

def save_to_iac_repo(merged_spec, output_filename, cluster_name):
    output_path = os.path.join(f'../AKS-Cluster-IaC/clusters/{cluster_name}/definitions', output_filename)
    if os.path.exists(output_path):
        logger.info(f"Cluster definition already exists: {output_path}")
        return

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as file:
        yaml.dump(merged_spec, file)
    logger.info(f"Cluster definition saved to: {output_path}")

def main():
    if len(sys.argv) != 3:
        logger.error("Usage: python automation.py <json_spec_path> <spec_name>")
        sys.exit(1)

    json_spec_path = sys.argv[1]
    spec_name = sys.argv[2]

    try:
        with open(json_spec_path, 'r') as file:
            json_spec = json.load(file)

        validate_spec(json_spec)

        env = json_spec.get('env', 'dev')
        cluster_name = json_spec.get('clusterName')
        template = load_template(spec_name, env)
        merged_spec = merge_spec_with_template(json_spec, template)

        output_filename = f"{cluster_name}-cluster-definition.yaml"
        save_to_iac_repo(merged_spec, output_filename, cluster_name)
    except FileNotFoundError as e:
        logger.error(e)
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON file: {json_spec_path}")
    except KeyError as e:
        logger.error(e)
    except ValueError as e:
        logger.error(e)

def validate_spec(spec):
    required_keys = ['clusterName', 'env', 'workerNodeCount', 'controlPlaneCount', 'region', 'spec', 'kubernetesVersion', 'networkPolicy', 'storageClass']
    for key in required_keys:
        if key not in spec:
            raise KeyError(f"Missing required key in spec: {key}")

if __name__ == "__main__":
    main()




import os
import sys
import json
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def process_json_specs(specs_dir, json_file, spec_name):
    json_spec_path = os.path.join(specs_dir, json_file)
    process_single_spec(json_spec_path, spec_name)

def process_single_spec(json_spec_path, spec_name):
    try:
        with open(json_spec_path, 'r') as file:
            json_spec = json.load(file)

        temp_json_path = 'temp_spec.json'
        with open(temp_json_path, 'w') as temp_file:
            json.dump(json_spec, temp_file)

        subprocess.run(['python', 'src/automation.py', temp_json_path, spec_name], check=True)
    except FileNotFoundError:
        logger.error(f"File not found: {json_spec_path}")
    except json.JSONDecodeError:
        logger.error(f"Error decoding JSON file: {json_spec_path}")
    except subprocess.CalledProcessError as e:
        logger.error(f"Error running automation script: {e}")

def main():
    if len(sys.argv) != 3:
        logger.error("Usage: python main.py <spec_name> <json_input_file>")
        sys.exit(1)
    
    spec_name = sys.argv[1]
    json_file = sys.argv[2]

    specs_dir = 'ClusterInputFolder'
    if not os.path.exists(specs_dir):
        logger.error(f"Specs directory does not exist: {specs_dir}")
        return

    process_json_specs(specs_dir, json_file, spec_name)

if __name__ == "__main__":
    main()
